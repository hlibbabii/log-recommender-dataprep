{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataprep.api.corpus as api\n",
    "import os\n",
    "from vocabstudy.common import PrepFunction, calc_and_display_stats, VocabStatsCsvWriter, HEADER, HOME\n",
    "import keyword\n",
    "PYTHON_KEYWORDS = keyword.kwlist\n",
    "\n",
    "PYTHON_DATASETS = ('rafael/python-minus-test',\n",
    "            'rafael/python-test'\n",
    "           )\n",
    "\n",
    "writer = VocabStatsCsvWriter(os.path.join(HOME, 'python-stats.csv'), HEADER)\n",
    "\n",
    "def run(prep_function: PrepFunction, description: str) -> None:\n",
    "    row = calc_and_display_stats(prep_function, description, PYTHON_DATASETS, PYTHON_KEYWORDS, 'py')\n",
    "    writer.write_line(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0-alpha.8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dataprep\n",
    "\n",
    "dataprep.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsplit, with comments and vocabulary. Filtering non-ascii.\n",
      "\n",
      "2019-08-19 00:26:42,684 [dataprep.infrastructure.stages] INFO: Checking first if vocabulary file exists: /home/lv71161/hlibbabii/.config/dataprep/1.0.0-alpha.8/vocab/python-minus-test_19-08-16T13-44-40_py_-_Uc10su/vocab\n",
      "2019-08-19 00:26:42,686 [dataprep.infrastructure.stages] INFO: Vocabulary is already computed and up-to-date\n",
      "2019-08-19 00:26:42,688 [dataprep.api.corpus] INFO: Preprocessed dataset is ready at /tmp/scratch/prep-datasets/python-minus-test_19-08-16T13-44-40_py_-_Uc10su_-_prep\n",
      "2019-08-19 00:27:22,697 [dataprep.infrastructure.stages] INFO: Checking first if vocabulary file exists: /home/lv71161/hlibbabii/.config/dataprep/1.0.0-alpha.8/vocab/python-test_15-07-11T19-08-23_py_-_Uc10su/vocab\n",
      "2019-08-19 00:27:22,700 [dataprep.infrastructure.stages] INFO: Vocabulary is already computed and up-to-date\n",
      "2019-08-19 00:27:22,701 [dataprep.api.corpus] INFO: Preprocessed dataset is ready at /tmp/scratch/prep-datasets/python-test_15-07-11T19-08-23_py_-_Uc10su_-_prep\n",
      "2019-08-19 00:27:23,704 [vocabstudy.common] DEBUG: Checking if train dataset exists: /tmp/scratch/prep-datasets/python-minus-test_19-08-16T13-44-40_py_-_Uc10su_-_prep\n",
      "2019-08-19 00:27:23,706 [vocabstudy.common] INFO: Train dataset is found and is going to be removed to save space: /tmp/scratch/prep-datasets/python-minus-test_19-08-16T13-44-40_py_-_Uc10su_-_prep. Removing ...\n",
      "2019-08-19 00:27:23,707 [vocabstudy.common] INFO: Moving test dataset from tmp storage: /tmp/scratch/prep-datasets/python-test_15-07-11T19-08-23_py_-_Uc10su_-_prep -> /home/lv71161/hlibbabii/prep-datasets\n",
      "2019-08-19 00:27:23,709 [vocabstudy.common] WARNING: Not moving. Path already exists: /home/lv71161/hlibbabii/prep-datasets/python-test_15-07-11T19-08-23_py_-_Uc10su_-_prep\n",
      "\n",
      "========================   Split example   =========================\n",
      "\n",
      "['void', 'test_WordUeberraschungPrinter', '(', ')', '{', '\\n', '\\t', 'if', '(', 'eps', '>', '=', '0.345e+4', ')', '{', '/', '/', 'FIXME', '10l', '\\n', '<EOL>', '\\t', '\\t', 'printWord', '(', '\"', '.', '.', '.', '<non-en>', '0x12', '\"', ')', ';', '\\n', '\\t', '}', '\\n', '}']\n",
      "\n",
      "========================   Stats           =========================\n",
      "\n",
      "Train vocab: 15493134\n",
      "Test vocab : 409380, identifiers: 367093 (0.90%)\n",
      "\n",
      "Train corpus size: 2863151236\n",
      "Test corpus size : 37368820, identifiers: 11315722 (0.30%)\n",
      "\n",
      "OOV in test set (top 9223372036854775807): 24447 (5.97%), oov tokens number: 97312 (0.26%)\n",
      "OOV in test set (top 200000): 305543 (74.64%), oov tokens number: 1082023 (2.90%)\n",
      "OOV in test set (top 100000): 342585 (83.68%), oov tokens number: 1389677 (3.72%)\n",
      "OOV in test set (top 75000): 354626 (86.63%), oov tokens number: 1529858 (4.09%)\n",
      "OOV in test set (top 50000): 368622 (90.04%), oov tokens number: 1756185 (4.70%)\n",
      "OOV in test set (top 25000): 387815 (94.73%), oov tokens number: 2146435 (5.74%)\n",
      "\n",
      "For identifiers: \n",
      "\n",
      "OOV in test set (top 9223372036854775807): 22946 (6.25%), oov tokens number: 94482 (0.83%)\n",
      "OOV in test set (top 200000): 280012 (76.28%), oov tokens number: 1044872 (9.23%)\n",
      "OOV in test set (top 100000): 312316 (85.08%), oov tokens number: 1336394 (11.81%)\n",
      "OOV in test set (top 75000): 323096 (88.01%), oov tokens number: 1471042 (13.00%)\n",
      "OOV in test set (top 50000): 336357 (91.63%), oov tokens number: 1692149 (14.95%)\n",
      "OOV in test set (top 25000): 348310 (94.88%), oov tokens number: 1981485 (17.51%)\n",
      "[5861936, 7454613, 1893531, 239006, 44048]\n"
     ]
    }
   ],
   "source": [
    "prep_function = PrepFunction(api.nosplit, [], {\"no_unicode\": True})\n",
    "description = \"unsplit, with comments and vocabulary. Filtering non-ascii.\"\n",
    "\n",
    "run(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering  comments ( + whitespace, + non-ascii)\n",
      "\n",
      "2019-08-19 00:28:09,552 [dataprep.infrastructure.stages] INFO: Checking first if vocabulary file exists: /home/lv71161/hlibbabii/.config/dataprep/1.0.0-alpha.8/vocab/python-minus-test_19-08-16T13-44-40_py_-_U0100u/vocab\n",
      "2019-08-19 00:28:09,553 [dataprep.infrastructure.stages] INFO: Vocabulary is already computed and up-to-date\n",
      "2019-08-19 00:28:09,554 [dataprep.api.corpus] INFO: Preprocessed dataset is ready at /tmp/scratch/prep-datasets/python-minus-test_19-08-16T13-44-40_py_-_U0100u_-_prep\n",
      "2019-08-19 00:28:48,016 [dataprep.infrastructure.stages] INFO: Checking first if vocabulary file exists: /home/lv71161/hlibbabii/.config/dataprep/1.0.0-alpha.8/vocab/python-test_15-07-11T19-08-23_py_-_U0100u/vocab\n",
      "2019-08-19 00:28:48,018 [dataprep.infrastructure.stages] INFO: Vocabulary is already computed and up-to-date\n",
      "2019-08-19 00:28:48,018 [dataprep.api.corpus] INFO: Preprocessed dataset is ready at /tmp/scratch/prep-datasets/python-test_15-07-11T19-08-23_py_-_U0100u_-_prep\n",
      "2019-08-19 00:28:48,993 [vocabstudy.common] DEBUG: Checking if train dataset exists: /tmp/scratch/prep-datasets/python-minus-test_19-08-16T13-44-40_py_-_U0100u_-_prep\n",
      "2019-08-19 00:28:48,994 [vocabstudy.common] INFO: Train dataset is found and is going to be removed to save space: /tmp/scratch/prep-datasets/python-minus-test_19-08-16T13-44-40_py_-_U0100u_-_prep. Removing ...\n",
      "2019-08-19 00:28:48,995 [vocabstudy.common] INFO: Moving test dataset from tmp storage: /tmp/scratch/prep-datasets/python-test_15-07-11T19-08-23_py_-_U0100u_-_prep -> /home/lv71161/hlibbabii/prep-datasets\n",
      "2019-08-19 00:28:48,996 [vocabstudy.common] WARNING: Not moving. Path already exists: /home/lv71161/hlibbabii/prep-datasets/python-test_15-07-11T19-08-23_py_-_U0100u_-_prep\n",
      "\n",
      "========================   Split example   =========================\n",
      "\n",
      "['void', 'test_WordUeberraschungPrinter', '(', ')', '{', 'if', '(', 'eps', '>', '=', '0.345e+4', ')', '{', '<comment>', 'printWord', '(', '\"', '.', '.', '.', '<non-en>', '0x12', '\"', ')', ';', '}', '}']\n",
      "\n",
      "========================   Stats           =========================\n",
      "\n",
      "Train vocab: 15207571\n",
      "Test vocab : 399500, identifiers: 358132 (0.90%)\n",
      "\n",
      "Train corpus size: 2037456204\n",
      "Test corpus size : 27787534, identifiers: 10134446 (0.36%)\n",
      "\n",
      "OOV in test set (top 9223372036854775807): 24081 (6.03%), oov tokens number: 95870 (0.35%)\n",
      "OOV in test set (top 200000): 297809 (74.55%), oov tokens number: 1047842 (3.77%)\n",
      "OOV in test set (top 100000): 333552 (83.49%), oov tokens number: 1341277 (4.83%)\n",
      "OOV in test set (top 75000): 345994 (86.61%), oov tokens number: 1471310 (5.29%)\n",
      "OOV in test set (top 50000): 359432 (89.97%), oov tokens number: 1683897 (6.06%)\n",
      "OOV in test set (top 25000): 378209 (94.67%), oov tokens number: 2048438 (7.37%)\n",
      "\n",
      "For identifiers: \n",
      "\n",
      "OOV in test set (top 9223372036854775807): 22651 (6.32%), oov tokens number: 93143 (0.92%)\n",
      "OOV in test set (top 200000): 273088 (76.25%), oov tokens number: 1012535 (9.99%)\n",
      "OOV in test set (top 100000): 304965 (85.15%), oov tokens number: 1292835 (12.76%)\n",
      "OOV in test set (top 75000): 315050 (87.97%), oov tokens number: 1413933 (13.95%)\n",
      "OOV in test set (top 50000): 328019 (91.59%), oov tokens number: 1623490 (16.02%)\n",
      "OOV in test set (top 25000): 339884 (94.90%), oov tokens number: 1896896 (18.72%)\n",
      "[5766902, 7332321, 1837961, 229356, 41031]\n"
     ]
    }
   ],
   "source": [
    "prep_function = PrepFunction(api.nosplit, [], {\"no_unicode\": True, \"no_spaces\": True, \"no_com\": True})\n",
    "description = \"Filtering  comments ( + whitespace, + non-ascii)\"\n",
    "\n",
    "run(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An additional choice, the model from Hellendoorn and Devanbu: keep strings shorter than 15 char (check SLP-core code), remove others, remove comments\n",
      "\n",
      "2019-08-19 00:29:40,522 [dataprep.infrastructure.stages] INFO: Checking first if vocabulary file exists: /home/lv71161/hlibbabii/.config/dataprep/1.0.0-alpha.8/vocab/python-minus-test_19-08-16T13-44-40_py_-_u0EFsu/vocab\n",
      "2019-08-19 00:29:40,523 [dataprep.infrastructure.stages] INFO: Parsing...\n",
      "2019-08-19 00:29:40,567 [dataprep.infrastructure.stages] INFO: Parsed dataset is up-to-date.\n",
      "2019-08-19 00:29:40,568 [dataprep.infrastructure.stages] INFO: Preprocessing...\n",
      "2019-08-19 00:29:40,569 [dataprep.to_repr] INFO: Reading parsed files from: /home/lv71161/hlibbabii/.cache/dataprep/1.0.0-alpha.8/parsed_datasets/python-minus-test_19-08-16T13-44-40_py\n",
      "2019-08-19 00:29:40,570 [dataprep.to_repr] INFO: Writing preprocessed files to /tmp/scratch/prep-datasets/python-minus-test_19-08-16T13-44-40_py_-_u0EFsu_-_prep\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 143569/1327894 [28:38<5:26:46, 60.40it/s] "
     ]
    }
   ],
   "source": [
    "prep_function = PrepFunction(api.nosplit, [], {\"no_com\": True, \"full_strings\": True, \"max_str_length\": 14})\n",
    "description = \"An additional choice, the model from Hellendoorn and Devanbu: keep strings shorter than 15 char (check SLP-core code), remove others, remove comments\"\n",
    "\n",
    "run(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.basic, [],\n",
    "                             {\"no_unicode\": True, \"no_spaces\": True, \"no_com\": True, \"max_str_length\": 14})\n",
    "description = \"[Unsplit, remove non-ascii, remove whitespace, remove comments, remove strings longer than 15 chars] + Word splitting via conventions\"\n",
    "\n",
    "run(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.basic, [],\n",
    "                             {\"no_unicode\": True, \"no_spaces\": True, \"no_com\": True, \"max_str_length\": 14,\n",
    "                              \"no_case\": True, \"split_numbers\": True, \"ronin\": True})\n",
    "description = \"[Unsplit, remove non-ascii, remove whitespace, remove comments, remove strings longer than 15 chars] + split via conventions, but keep case. + Split numbers + Ronin\"\n",
    "run(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.bpe, ['python-bpe-training_nounicode-2000'], {\"no_com\": True, \"max_str_length\": 14, \"no_unicode\": True})\n",
    "description = \"bpe 2k Hellendoorn and Devanbu strings no comments no unicode \"\n",
    "run(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.bpe, ['python-bpe-training_nounicode-5000'], {\"no_com\": True, \"max_str_length\": 14, \"no_unicode\": True})\n",
    "description = \"bpe 5k Hellendoorn and Devanbu strings no comments no unicode\"\n",
    "run(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.bpe, ['python-bpe-training_nounicode-10000'], {\"no_com\": True, \"max_str_length\": 14, \"no_unicode\": True})\n",
    "description = \"bpe 10k Hellendoorn and Devanbu strings no comments no unicode\"\n",
    "run(prep_function, description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
