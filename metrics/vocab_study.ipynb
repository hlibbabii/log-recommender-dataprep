{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/lv71161/hlibbabii/projects/log-recommender-dataprep/')\n",
    "\n",
    "import os\n",
    "\n",
    "import dataprep.api.corpus as api\n",
    "import dataprep.api.text as text_api\n",
    "from metrics.vector import get_new_vocab\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Callable, Tuple, Union, Any, List, Dict\n",
    "\n",
    "from dataprep.api.corpus import PreprocessedCorpus\n",
    "import logging\n",
    "import regex\n",
    "import pandas as pd\n",
    "\n",
    "IDENTIFIER_REGEX = \"([[:lower:]]|[[:upper:]]|$|_)([[:lower:]]|[[:upper:]]|[0-9]|_|$)*\"\n",
    "\n",
    "KEYWORDS = { \"abstract\", \"assert\", \"boolean\", \"break\", \"byte\", \"case\", \"catch\", \"char\",\n",
    "\t\t\t\"class\", \"const\", \"continue\", \"default\", \"do\", \"double\", \"else\", \"enum\", \"extends\", \"final\", \"finally\",\n",
    "\t\t\t\"float\", \"for\", \"goto\", \"if\", \"implements\", \"import\", \"instanceof\", \"int\", \"interface\", \"long\", \"native\",\n",
    "\t\t\t\"new\", \"package\", \"private\", \"protected\", \"public\", \"return\", \"short\", \"static\", \"strictfp\", \"super\",\n",
    "\t\t\t\"switch\", \"synchronized\", \"this\", \"throw\", \"throws\", \"transient\", \"try\", \"void\", \"volatile\", \"while\",\n",
    "\t\t\t\"true\", \"false\", \"null\" };\n",
    "\n",
    "def is_identifier(word: str) -> bool:\n",
    "    if word not in KEYWORDS and regex.fullmatch(IDENTIFIER_REGEX, word):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def without_non_identifiers(dct: Dict[str, int]) -> Dict[str, int]:\n",
    "    res = {}\n",
    "    for k, v in dct.items():\n",
    "        if is_identifier(k):\n",
    "            res[k] = v\n",
    "    return res   \n",
    "\n",
    "#logging.disable(logging.WARNING)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Corpus(object):\n",
    "    path: str\n",
    "    extensions: str  # in format \"py\" or \"java|c|py\"\n",
    "        \n",
    "\n",
    "PrepCallable = Callable[..., PreprocessedCorpus]\n",
    "ParametrizedPrepCallable = Callable[[Corpus], PreprocessedCorpus]\n",
    "        \n",
    "@dataclass(frozen=True)\n",
    "class PrepFunction(object):\n",
    "    callable: PrepCallable\n",
    "    params: List = field(default_factory=list)\n",
    "    options: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "    @property\n",
    "    def apply(self) -> ParametrizedPrepCallable:\n",
    "        def prep_corpus(corpus: Corpus, **kwargs):\n",
    "            return self.callable(corpus.path, *self.params, **self.options, **kwargs,\n",
    "                                 calc_vocab=True, extensions=corpus.extensions)\n",
    "\n",
    "        return prep_corpus\n",
    "\n",
    "HOME='/home/lv71161/hlibbabii'\n",
    "PATH_TO_DATASETS=os.path.join(HOME, 'raw_datasets')\n",
    "datasets = [\n",
    "    'allamanis/java-minus-small-test',\n",
    "            'allamanis/java-small-test'\n",
    "           ]\n",
    "\n",
    "\n",
    "sample='''void test_WordUeberraschungPrinter() {\n",
    "    if (eps >= 0.345e+4) { // FIXME 10L\n",
    "        printWord(\"     ...     Ãœberraschung 0x12\");\n",
    "    }\n",
    "}'''\n",
    "\n",
    "\n",
    "def print_oov_stats(train_vocab, test_vocab):\n",
    "    tops = [sys.maxsize, 200000, 100000, 75000, 50000, 25000]\n",
    "    for top in tops:\n",
    "        test_tokens_number = sum(test_vocab.values())\n",
    "        oov_in_test = get_new_vocab(list(train_vocab.items())[:top], list(test_vocab.items()))\n",
    "        oov_tokens_number = sum(map(lambda e: e[1], oov_in_test))\n",
    "        print(f\"OOV in test set (top {top}): {len(oov_in_test)} ({100.0 * len(oov_in_test) / len(test_vocab):.2f}%), oov tokens number: {oov_tokens_number} ({100.0*oov_tokens_number/test_tokens_number:.2f}%)\")\n",
    "\n",
    "\n",
    "def chunks(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i * k + min(i, m):(i + 1) * k + min(i + 1, m)] for i in range(n))\n",
    "        \n",
    "\n",
    "def percentiles(vocab: Dict[str, int]) -> Tuple[List[float], List[int]]:\n",
    "    N_BUCKETS = 100\n",
    "    values_list = list(vocab.values())\n",
    "    lsts = chunks(values_list, N_BUCKETS)\n",
    "    avg = [pd.Series(sequence).mean() for sequence in lsts]\n",
    "    lsts = chunks(values_list, N_BUCKETS)\n",
    "    med = [sequence[len(sequence) // 2] for sequence in lsts]\n",
    "    return avg, med\n",
    "\n",
    "\n",
    "def which_bucket(what: int, buckets: List[int]):\n",
    "    for i in range(len(buckets)):\n",
    "        if (i+1 == len(buckets) or what < buckets[i+1]) and what >= buckets[i]:\n",
    "            return i\n",
    "    raise ValueError(f'Invalid value: {what}, buckets: {buckets}')\n",
    "\n",
    "def frequencies(vocab: Dict[str, int], buckets: List[Tuple[int]]):\n",
    "    counts = [0 for _ in buckets]\n",
    "    for k, v in vocab.items():\n",
    "        counts[which_bucket(v, buckets)] += 1\n",
    "    return counts\n",
    "    \n",
    "\n",
    "def plot_percentiles(vocab: Dict[str, int]) -> None:\n",
    "    import plotly.graph_objects as go\n",
    "        \n",
    "    avg, med = percentiles(vocab)\n",
    "    print(\"Average:\")\n",
    "    print(avg)\n",
    "    df = pd.DataFrame({'percentile': list(range(100)), 'n_tokens': avg})\n",
    "    \n",
    "    data = [go.Bar(x=df.percentile, y=df.n_tokens)]\n",
    "    fig = go.Figure(data=data)\n",
    "    fig.update_layout(yaxis_type=\"log\")\n",
    "    fig.show()\n",
    "    \n",
    "    print(\"Median:\")\n",
    "    print(med)\n",
    "    df = pd.DataFrame({'percentile': list(range(100)), 'n_tokens': med})\n",
    "    \n",
    "    data = [go.Bar(x=df.percentile, y=df.n_tokens)]\n",
    "    fig = go.Figure(data=data)\n",
    "    fig.update_layout(yaxis_type=\"log\")\n",
    "    fig.show()\n",
    "\n",
    "    \n",
    "def get_corpus_size(vocab: Dict[str, int]) -> int:\n",
    "    return sum(vocab.values())\n",
    "\n",
    "import shutil\n",
    "        \n",
    "def calc_and_display_stats(prep_function, description) -> None:    \n",
    "    print(f\"{description}\\n\")\n",
    "    \n",
    "    vocabs = []\n",
    "    prep_corpora = []\n",
    "    for dataset in datasets:\n",
    "        dataset_path = os.path.join(PATH_TO_DATASETS, dataset)\n",
    "        prep_corpus = prep_function.apply(Corpus(dataset_path, 'java'), output_path=os.path.join(HOME, 'prep-datasets'))\n",
    "        prep_corpora.append(prep_corpus)\n",
    "        vocabs.append(prep_corpus.load_vocab())\n",
    "    print(f'Removing prepped dataset at {prep_corpora[0].path_to_prep_dataset}')\n",
    "    shutil.rmtree(prep_corpora[0].path_to_prep_dataset)\n",
    "    \n",
    "    train_vocab, test_vocab = tuple(vocabs)\n",
    "    print(\"\\n========================   Split example   =========================\\n\")\n",
    "    \n",
    "    text_callable = getattr(text_api, prep_function.callable.__name__)\n",
    "    text_prep_func = PrepFunction(text_callable, prep_function.params, prep_function.options)\n",
    "    print(text_callable(sample, *prep_function.params, **prep_function.options, extension=\"java\"))\n",
    "\n",
    "    print(\"\\n========================   Stats           =========================\\n\")\n",
    "\n",
    "\n",
    "    print(f\"Train vocab: {len(train_vocab)}\")\n",
    "    test_vocab_identifiers = without_non_identifiers(test_vocab)\n",
    "    \n",
    "    print(f\"Test vocab : {len(test_vocab)}, identifiers: {len(test_vocab_identifiers)} ({len(test_vocab_identifiers)/len(test_vocab):.2f}%)\\n\")\n",
    "  \n",
    "    print(f\"Train corpus size: {get_corpus_size(train_vocab)}\")\n",
    "    test_tokens_number_ids = get_corpus_size(test_vocab_identifiers)\n",
    "    test_tokens_number = get_corpus_size(test_vocab)\n",
    "    print(f\"Test corpus size : {test_tokens_number}, identifiers: {test_tokens_number_ids} ({test_tokens_number_ids/test_tokens_number:.2f}%)\\n\")\n",
    "    \n",
    "    print_oov_stats(train_vocab, test_vocab)\n",
    "    print(\"\\nFor identifiers: \\n\")\n",
    "    print_oov_stats(train_vocab, test_vocab_identifiers)\n",
    "    \n",
    "    #plot_percentiles(train_vocab)\n",
    "    buckets = [1, 2, 11, 101, 1001]\n",
    "    freqs = frequencies(train_vocab, buckets)\n",
    "    print(freqs)\n",
    "    return 0,0 #TODO remove this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.0-alpha.8'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dataprep\n",
    "dataprep.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.nosplit)\n",
    "description = \"unsplit, with comments and vocabulary. No filtering.\"\n",
    "\n",
    "nosplit_train_prep, nosplit_test_prep = calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.nosplit, [], {\"no_unicode\": True})\n",
    "description = \"Filtering non-ASCII tokens\"\n",
    "\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.nosplit, [], {\"no_unicode\": True, \"no_spaces\": True})\n",
    "description = \"Filtering whitespace (+ non-ascii)\"\n",
    "\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.nosplit, [], {\"no_unicode\": True, \"no_spaces\": True, \"no_com\": True})\n",
    "description = \"Filtering comments (+ whitespace, + non-ascii)\"\n",
    "\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.nosplit, [], {\"no_unicode\": True, \"no_spaces\": True, \"no_com\": True, \"no_str\": True})\n",
    "description = \"Filtering  strings (+ comments, + whitespace, + non-ascii)\"\n",
    "\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.nosplit, [], {\"no_com\": True, \"full_strings\": True, \"max_str_length\": 14})\n",
    "description = \"An additional choice, the model from Hellendoorn and Devanbu: keep strings shorter than 15 char (check SLP-core code), remove others, remove comments\"\n",
    "\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.basic, [], {\"no_unicode\": True, \"no_spaces\": True, \"no_com\": True, \"max_str_length\": 14})\n",
    "description = \"[Unsplit, remove non-ascii, remove whitespace, remove comments, remove strings longer than 15 chars] + Word splitting via conventions\"\n",
    "\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.basic, [], {\"no_unicode\": True, \"no_spaces\": True, \"no_com\": True, \"max_str_length\": 14, \"no_case\": True})\n",
    "description = \"[Unsplit, remove non-ascii, remove whitespace, remove comments, remove strings longer than 15 chars] + Word splitting via conventions, and remove case.\"\n",
    "\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.basic, [], {\"no_unicode\": True, \"no_spaces\": True, \"no_com\": True, \"max_str_length\": 14, \"no_case\": True, \"split_numbers\": True})\n",
    "description = \"[Unsplit, remove non-ascii, remove whitespace, remove comments, remove strings longer than 15 chars] + split via conventions, but keep case.+ Split numbers\"\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.basic, [], {\"no_unicode\": True, \"no_spaces\": True, \"no_com\": True, \"max_str_length\": 14, \"no_case\": True, \"split_numbers\": True, \"ronin\": True})\n",
    "description = \"[Unsplit, remove non-ascii, remove whitespace, remove comments, remove strings longer than 15 chars] + split via conventions, but keep case. + Split numbers + Ronin\"\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"{datasets[0]}\"\n",
    "\n",
    "dataprep learn-bpe 20000 -p \"$1\" --id \"main\" -e \"java\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.bpe, ['java-bpe-training_nounicode-1000'], {\"no_com\": True, \"no_str\": True})\n",
    "description = \"bpe 1k no strings no comments\"\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.bpe, ['java-bpe-training_nounicode-2000'], {\"no_com\": True, \"no_str\": True})\n",
    "description = \"bpe 2k no strings no comments\"\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.bpe, ['java-bpe-training_nounicode-5000'], {\"no_com\": True, \"no_str\": True})\n",
    "description = \"bpe 5k no strings no comments\"\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.bpe, ['java-bpe-training_nounicode-10000'], {\"no_com\": True, \"no_str\": True})\n",
    "description = \"bpe 10k no strings no comments\"\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.bpe, ['java-bpe-training_nounicode-20000'], {\"no_com\": True, \"no_str\": True})\n",
    "description = \"bpe 20k no strings no comments\"\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.bpe, ['java-bpe-training_nounicode-1000'], {\"no_com\": True, \"max_str_length\": 14})\n",
    "description = \"bpe 1k Hellendoorn and Devanbu strings no comments\"\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.bpe, ['java-bpe-training_nounicode-2000'], {\"no_com\": True, \"max_str_length\": 14})\n",
    "description = \"bpe 2k Hellendoorn and Devanbu strings no comments\"\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.bpe, ['java-bpe-training_nounicode-5000'], {\"no_com\": True, \"max_str_length\": 14})\n",
    "description = \"bpe 5k Hellendoorn and Devanbu strings no comments\"\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.bpe, ['java-bpe-training_nounicode-10000'], {\"no_com\": True, \"max_str_length\": 14})\n",
    "description = \"bpe 10k Hellendoorn and Devanbu strings no comments\"\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.bpe, ['java-bpe-training_nounicode-20000'], {\"no_com\": True, \"max_str_length\": 14})\n",
    "description = \"bpe 20k Hellendoorn and Devanbu strings no comments\"\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.bpe, ['java-bpe-training_nounicode-1000'], {\"no_com\": True})\n",
    "description = \"bpe 1k no comments\"\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.bpe, ['java-bpe-training_nounicode-2000'], {\"no_com\": True})\n",
    "description = \"bpe 2k no comments\"\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.bpe, ['java-bpe-training_nounicode-5000'], {\"no_com\": True})\n",
    "description = \"bpe 5k no comments\"\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.bpe, ['java-bpe-training_nounicode-10000'], {\"no_com\": True})\n",
    "description = \"bpe 10k no comments\"\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.bpe, ['java-bpe-training_nounicode-20000'], {\"no_com\": True})\n",
    "description = \"bpe 20k no comments\"\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.bpe, ['java-bpe-training_nounicode-1000'], {})\n",
    "description = \"bpe 1k\"\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.bpe, ['java-bpe-training_nounicode-2000'], {})\n",
    "description = \"bpe 2k\"\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.bpe, ['java-bpe-training_nounicode-5000'], {})\n",
    "description = \"bpe 5k\"\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.bpe, ['java-bpe-training_nounicode-10000'], {})\n",
    "description = \"bpe 10k\"\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_function = PrepFunction(api.bpe, ['java-bpe-training_nounicode-20000'], {})\n",
    "description = \"bpe 20k\"\n",
    "calc_and_display_stats(prep_function, description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metrics",
   "language": "python",
   "name": "metrics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
